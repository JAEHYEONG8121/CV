{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0v4-4cHwN5z9"
      },
      "source": [
        "Given code: \n",
        "1. import packages\n",
        "2. load image pairs img1 & img2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "UxIiSV-yN5jv"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load image pairs - you can change file names\n",
        "image1 = cv2.imread(\"IMG_7577.png\")\n",
        "image2 = cv2.imread(\"IMG_7578.png\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GS-5N7zbN5Rs"
      },
      "source": [
        "Problem 1: [code by yourself]\n",
        "\n",
        "Define a function `get_transform_from_keypoints()` that computes the 3x3 homogeneous transform H from keypoint matches with the following input parameters and return variables:\n",
        "* img_src (input): image that is warped to be aligned to img_dst \n",
        "* img_dst (input): reference image to align img_src\n",
        "* H (output/return): computed linear transform matrix\n",
        "* kpts_src (output/return): computed keypoints for img_src\n",
        "* dscrpt_src (output/return): computed descriptors for img_src\n",
        "* kpts_dst (output/return): computed keypoints for img_dst\n",
        "* dscrpt_dst (output/return): computed descriptors for img_dst\n",
        "* matches (output/return): keypoint matches determined from SIFT\n",
        "\n",
        "When computing the linear transform, follow this proceses:\n",
        "* detect SIFT keypoints and compute SIFT descriptors\n",
        "* find the linear transform matrix H using the matched keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "9IheSHuUN5CR"
      },
      "outputs": [],
      "source": [
        "def get_transform_from_keypoints(img_src, img_dst):\n",
        "    img_src_gray = cv2.cvtColor(img_src, cv2.COLOR_BGR2GRAY)\n",
        "    img_dst_gray = cv2.cvtColor(img_dst, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Create a SIFT object and detect keypoints and descriptors for each image\n",
        "    sift = cv2.SIFT_create()\n",
        "    kpts_src, dscrpt_src = sift.detectAndCompute(img_src_gray, None)\n",
        "    kpts_dst, dscrpt_dst = sift.detectAndCompute(img_dst_gray, None)\n",
        "    #print(dscrpt_src, dscrpt_dst)\n",
        "\n",
        "    # Match the descriptors using Brute-Force matcher\n",
        "    bf = cv2.BFMatcher()\n",
        "    matches = bf.match(dscrpt_src, dscrpt_dst)\n",
        "\n",
        "    # Sort the matches in the order of their distances\n",
        "    matches = sorted(matches, key=lambda x: x.distance)\n",
        "\n",
        "    # Find the homography matrix using the matched keypoints\n",
        "    src_pts = np.float32([kpts_src[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
        "    dst_pts = np.float32([kpts_dst[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
        "    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 2.0)\n",
        "    matches = [matches[i] for i in range(len(matches)) if mask[i]]\n",
        "\n",
        "    # Return outputs\n",
        "    return H, kpts_src, dscrpt_src, kpts_dst, dscrpt_dst, matches"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VLa2-PdCN4sj"
      },
      "source": [
        "Problem 2: [Code by yourself]\n",
        "\n",
        "Define a function named `stitch_image()` that generates a stitched image from img_ref, img_align, and T, with the following input parameters and return variables:\n",
        "* img_src (input): image that is warped to be aligned to img_dst \n",
        "* img_dst (input): reference image to align img_src\n",
        "* H (input): computed linear transform matrix\n",
        "* stitched_image (output): the stitched image\n",
        "\n",
        "This function should include the following processes:\n",
        "* Compute the size of the output stitched image and the offset that ensures all pixels from both images are included in the stitched image\n",
        "* Modify H to account for the image offset\n",
        "* Warp the first src image into the second image\n",
        "* Blend in the second dst image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "j-eV8gu5NwAC"
      },
      "outputs": [],
      "source": [
        "def get_stitched_image(img_src, img_dst, H):\n",
        "    # Compute the size of the output stitched image\n",
        "    rows_src, cols_src = img_src.shape[:2]\n",
        "    rows_dst, cols_dst = img_dst.shape[:2]\n",
        "  \n",
        "    # 첫 번째 이미지 꼭지점 좌표 생성\n",
        "    corners_src = np.float32([[0, 0], [0, rows_src], [cols_src, rows_src], [cols_src, 0]]).reshape(-1, 1, 2)\n",
        "  \n",
        "    # H변환행렬을 통한 1st 이미지의 꼭지점좌표를 2nd 이미지의 좌표공간으로 변환\n",
        "    corners_dst = cv2.perspectiveTransform(corners_src, H)\n",
        "    \n",
        "    # 2nd 이미지의 꼭지점 좌표와 1st 이미지의 꼭지점 좌표 합치기\n",
        "    corners = np.concatenate((corners_dst, corners_src), axis=0)\n",
        "    \n",
        "    # 변환된 자표들 중 최대/최소 x, y좌표 구하기\n",
        "    [x_min, y_min] = np.int32(corners.min(axis=0).ravel() - 0.5)\n",
        "    [x_max, y_max] = np.int32(corners.max(axis=0).ravel() + 0.5)\n",
        "    \n",
        "    # 음수값을 보정하기 위한 offset 설정\n",
        "    offset_x = -x_min if x_min < 0 else 0\n",
        "    offset_y = -y_min if y_min < 0 else 0\n",
        "    \n",
        "    # 출력 이미지 너비와 높이 계산\n",
        "    dst_width = max(x_max, cols_dst) - min(x_min, 0)\n",
        "    dst_height = max(y_max, rows_dst) - min(y_min, 0)\n",
        "\n",
        "    # Modify H to account for the image offset - 변환행렬 오프셋 고려\n",
        "    H_modified = np.array([[1, 0, offset_x], [0, 1, offset_y], [0, 0, 1]]) @ H\n",
        "  \n",
        "    # Warp the first image to the perspective of the second image\n",
        "    # -> img_src를 2nd 이미지의 원근 변환에 맞게 변환\n",
        "    warped_img = cv2.warpPerspective(img_src, H_modified, (dst_width, dst_height))\n",
        "\n",
        "    # Alpha blending\n",
        "    alpha = 0  # 가중치 값 (조절 가능)\n",
        "    stitched_image = np.zeros((dst_height, dst_width, 3), dtype=np.uint8)\n",
        "    stitched_image[offset_y:rows_dst + offset_y, offset_x:cols_dst + offset_x] = img_dst\n",
        "\n",
        "    # warped_img 그레이 스케일로 변환 & 임계값 기준으로 이진화하여 mask_warped 생성\n",
        "    mask_warped = cv2.cvtColor(warped_img, cv2.COLOR_BGR2GRAY)\n",
        "    ret, mask_warped = cv2.threshold(mask_warped, 1, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # mask_dst 생성\n",
        "    mask_dst = cv2.bitwise_not(mask_warped)\n",
        "\n",
        "    # 위에서 생성된 두 마스크 조합\n",
        "    mask_combined = cv2.bitwise_and(mask_warped, mask_dst)\n",
        "\n",
        "    # 각 이미지에 마스크를 적용하여 일부 영역을 검은색으로 만듬\n",
        "    stitched_image = cv2.bitwise_and(stitched_image, cv2.cvtColor(mask_dst, cv2.COLOR_GRAY2BGR))\n",
        "    warped_img = cv2.bitwise_and(warped_img, cv2.cvtColor(mask_warped, cv2.COLOR_GRAY2BGR))\n",
        "    \n",
        "    # stiched_image와 warped image를 알파 블렌딩 수행\n",
        "    stitched_image = cv2.add(stitched_image, warped_img)\n",
        "\n",
        "    # return output\n",
        "    return stitched_image\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "94CDChs3gh5V"
      },
      "source": [
        "Given code: \n",
        "3. Call function `get_transform_from_keypoints` - detect and match keypoints and compute transform\n",
        "4. Draw the matches on a new image to check validity of matched keypoints\n",
        "5. Call function `get_stitched_image` - create stitched image and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "hU2wERZJgiDq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3. Detect and match keypoints and compute transform\n",
        "H, kpts_src, _, kpts_dst, _, matches = get_transform_from_keypoints(image1, image2)\n",
        "\n",
        "# 4. Draw the matches on a new image to check validity of matched keypoints\n",
        "match_image = cv2.drawMatches(image1, kpts_src, image2, kpts_dst, matches, None)\n",
        "cv2.imwrite('matches.png', match_image)\n",
        "\n",
        "# 3. Create stitched image and save\n",
        "stitched_image = get_stitched_image(image1, image2, H)\n",
        "cv2.imwrite('stitched.png', stitched_image)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Machine Learning",
      "language": "python",
      "name": "conda"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
